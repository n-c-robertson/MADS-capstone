{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d77df0-384f-4f01-b758-0776c2b78fdf",
   "metadata": {},
   "source": [
    "# Building an Image Classifier\n",
    "\n",
    "Author: Nathan Robertson\n",
    "\n",
    "A core underlying hypothesis of this project is: **if** we can classify the luxury of a Zillow listing based on how it looks, **then** we will build a significantly better predicter, **because** images tell a story that text data can't. A 3 bed - 2 bath ranch can look like a dump, or it can look like a million dollar home. A human being can look at a series of images and quickly make this determination, but a human can't look at _millions_ of images. So we need to find a way to train a convolutional neural network that can do this work for us.\n",
    "\n",
    "As this is a graduate school portfolio project, time and funding are limited. I can't label these pictures all by hand, nor can I pay others to do it for me. So instead, we're going to try leveraging off the shelf AI tools to help generate the training dataset. From there, we will build a convolutional neural network from scratch with two layers. The first layer will predict what type of room the picture is (example: bedroom, bathroom, living room), and the second layer will be a series of neural networks for each room type to rank its perceived quality (with a 10 being outstanding quality, and a 1 being horrible quality).\n",
    "\n",
    "There are convolutional neural networks out there that could handle the first stage - but this is a graduate school project, so let's have fun and build this pizza from scratch. Besides: the second layer (a room type-specific determination of quality on a 1-10 scale) is not something that exists off the shelf, so we'd still need to build that.\n",
    "\n",
    "The end output of this will be a series of 50,000 - 100,000 photos with predicted room types and quality. These will be used to train the neural network that classifies the 6~ million photos in the data set.\n",
    "\n",
    "### Step 0: Import packages.\n",
    "\n",
    "Import packages that will handle.\n",
    "\n",
    "* Data manipulation.\n",
    "* Interacting with OpenAI's API.\n",
    "* Visualizing and extracting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235949e6-ca07-4024-a667-88b08a4ed194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation.\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# AI for autogenerating image labels.\n",
    "import openai\n",
    "client = openai.OpenAI(api_key='')\n",
    "\n",
    "# Visualizing images\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Handling image extraction.\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Handle saving images\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For progress tracking.\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Connecting to Microsoft Azure.\n",
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "key = \"\"\n",
    "endpoint = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eafbb3-ba33-404f-b49c-e855ef6e913a",
   "metadata": {},
   "source": [
    "### Step 1: Melt data into new DataFrame\n",
    "\n",
    "Let's take the existing data from Zillow webscraping, and read it in. We'll then isolate the list of images for each listing, and melt that into a new DataFrame with one row for each unique image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ca39e6-a3a2-406f-9d7c-a74d5412edee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0l/38yb605j6z30lbrmlsfhrq0h0000gn/T/ipykernel_63489/1142338921.py:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('data/backup/BACKUP zillow_listing_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# Read in data.\n",
    "df = pd.read_csv('BACKUP zillow_listing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce832dc-d060-4494-a29e-5a0af78071d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of photos in dataset: 6138344\n"
     ]
    }
   ],
   "source": [
    "# How many photos are there?\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # If there is at least one photo.\n",
    "    try:\n",
    "        # Count each photo.\n",
    "        for photo in ast.literal_eval(row['photosList']):\n",
    "            count += 1\n",
    "    # If there are no photos, skip.\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print('# of photos in dataset:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a702d243-5e91-4504-bc2a-949fbef2879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "melt_photo_data\n",
    "\n",
    "Turn the original webscraped Zillow DataFrame into a melted two column DataFrame of images.\n",
    "\n",
    "    Args:\n",
    "      already_generated: if True, skip the function and just load the data from a prior run.\n",
    "      \n",
    "    Returns:\n",
    "      A melted DataFrame of Zillow images, including the Zillow listing ID the photo came from\n",
    "      and the URL of where the photo lives on Zillow's servers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def melt_photo_data(already_generated=False):\n",
    "\n",
    "    if already_generated == False:\n",
    "        # Set the chunk size\n",
    "        chunk_size = 1000\n",
    "        \n",
    "        # Empty lists for capturing results.\n",
    "        zillowIds = []\n",
    "        photos = []\n",
    "        \n",
    "        # Chunk and process the data\n",
    "        for chunk_start in tqdm(range(0, df.shape[0], chunk_size), desc=\"Processing chunks\"):\n",
    "            \n",
    "            chunk_end = min(chunk_start + chunk_size, df.shape[0])\n",
    "            chunk_df = df.iloc[chunk_start:chunk_end]\n",
    "        \n",
    "            for i, row in chunk_df.iterrows():\n",
    "                # If there is at least one photo.\n",
    "                try:\n",
    "                    # Count each photo.\n",
    "                    for photo in ast.literal_eval(row['photosList']):\n",
    "                        zillowIds.append(row['zillowId'])\n",
    "                        photos.append(photo)\n",
    "                        \n",
    "                # If there are no photos, skip.\n",
    "                except:\n",
    "                    pass\n",
    "        \n",
    "        # Create a two-column DataFrame of all zillowId / photo URL combinations.\n",
    "        photos_df = pd.DataFrame(data={\n",
    "            'zillowId': zillowIds,\n",
    "            'photo': photos\n",
    "        })\n",
    "        \n",
    "        photos_df.to_csv('BACKUP listing_photos_dataset.csv', index=False)\n",
    "    \n",
    "        return photos_df\n",
    "\n",
    "    elif already_generated == True:\n",
    "        return pd.read_csv('BACKUP listing_photos_dataset.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf9fcf8-9f4c-47ac-898f-4dabcb084b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataFrame.\n",
    "photos_df = melt_photo_data(already_generated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d4f0b",
   "metadata": {},
   "source": [
    "Taking a look at the results, we can see one row for each image. The attributes are the Zillow listing ID from where the photo came, and a URL to where that photo lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cba820cf-59ce-4891-9089-96b1a8d339a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zillowId</th>\n",
       "      <th>photo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2091638544</td>\n",
       "      <td>https://photos.zillowstatic.com/fp/7ea447eb095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2091638544</td>\n",
       "      <td>https://photos.zillowstatic.com/fp/de4b606d805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2091638544</td>\n",
       "      <td>https://photos.zillowstatic.com/fp/ac0607cb7db...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2091638544</td>\n",
       "      <td>https://photos.zillowstatic.com/fp/aec8b02cdf3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2091638544</td>\n",
       "      <td>https://photos.zillowstatic.com/fp/02b2a5d3d0a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     zillowId                                              photo\n",
       "0  2091638544  https://photos.zillowstatic.com/fp/7ea447eb095...\n",
       "1  2091638544  https://photos.zillowstatic.com/fp/de4b606d805...\n",
       "2  2091638544  https://photos.zillowstatic.com/fp/ac0607cb7db...\n",
       "3  2091638544  https://photos.zillowstatic.com/fp/aec8b02cdf3...\n",
       "4  2091638544  https://photos.zillowstatic.com/fp/02b2a5d3d0a..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the results.\n",
    "photos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a433abb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://photos.zillowstatic.com/fp/7ea447eb0954cacc829420178ead5fe4-p_d.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photos_df['photo'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0628361-f3b9-4dd6-a227-f2d4e90d4393",
   "metadata": {},
   "source": [
    "### Step 2: Create automated image labeler\n",
    "\n",
    "I attempted to label images myself. Using Streamlit, I got into a pretty good workflow...but I couldn't label more than 500 images an hour. Time is limited, and I unfortunately don't have the bandwidth to label these images with the human care I'd like. But, labeling a thousand or so images gave me a decent sense for what type of rules I might pass along to a more automated solution.\n",
    "\n",
    "So instead, we're going to leverage ChatGPT and Microsoft Azure Vision AI. With a little bit of elbow grease on the prompt engineering, we can get something that is a satisfactory quality up and running that can generate room label and room quality rankings for a little about $0.01 USD / image.\n",
    "\n",
    "**The flow**\n",
    "\n",
    "* First, `azure.ai.vision.imagenaalysis` will generate a description of the image.\n",
    "* Then `gpt-4o` will look at the description and determine what type of room it is from a list of options I created.\n",
    "* Using `gpt-4o`, and some of the listing metadata where appropriate, we're going to convert that description into a 1-10 numerical rating of the quality of the listing.\n",
    "\n",
    "### Step 2.1 Define ChatGPT / Azure AI API connections\n",
    "\n",
    "We're going to create the 3 functions we will use to interact with ChatGPT and Azure AI to classify the room, describe its quality, and generate a ranking.\n",
    "\n",
    "The first one is fairly straightforward. We're going to pass a list of potential room types into a prompt. `gpt-4-vision-preview`'s job is to determine which room type it thinks best represents the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a96ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "azure_describe_room\n",
    "\n",
    "Look at an image, and generate a description of it.\n",
    "\n",
    "    Args:\n",
    "        url: The url to the Zillow listing photo being assessed.\n",
    "        \n",
    "    Returns:\n",
    "        A result object dictionary that contains all of the information about the picture.\n",
    "\"\"\"\n",
    "\n",
    "def azure_describe_room(url):\n",
    "    # Create an Image Analysis client\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureKeyCredential(key)\n",
    "    )\n",
    "    \n",
    "    # Extract the visual features from the url.\n",
    "    result = client.analyze_from_url(\n",
    "        image_url=url,\n",
    "        visual_features=[VisualFeatures.TAGS,\n",
    "            VisualFeatures.CAPTION,\n",
    "            VisualFeatures.DENSE_CAPTIONS\n",
    "            ]\n",
    "    )\n",
    "    \n",
    "    # Return the result.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1389386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "unpack_azure_results\n",
    "\n",
    "Look at a description generated by azure_describe_room, and pull out the relevant information\n",
    "into a condensed set of text that can be fed to ChatGPT.\n",
    "\n",
    "    Args:\n",
    "        result: the result object generated by the function `azure_describe_room`.\n",
    "    \n",
    "    Returns:\n",
    "        A string object with the minimal information required to pass onto ChatGPT.\n",
    "\"\"\"\n",
    "\n",
    "def unpack_azure_results(result):\n",
    "    \n",
    "    # Placeholder strings.\n",
    "    caption = '[]'\n",
    "    denseCaption = '[]'\n",
    "    tags = '[]'\n",
    "\n",
    "    # For each string, attempt to extract the information. If it fails for any reason, pass.\n",
    "    # Passing means the empty value of '[]' will be returned for that portion of the final string.\n",
    "    try:\n",
    "        caption = result['captionResult']['text']\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        denseCaption = '; '.join([denseCaption['text'] for denseCaption in result['denseCaptionsResult']['values']])\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        tags = ', '.join([tag['name'] for tag in result['tagsResult']['values']])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Build final string.\n",
    "    full_description = f\"\"\"*caption* {caption}. *denseCaptions* {denseCaption}. *tags* {tags}.\"\"\"\n",
    "\n",
    "    # Return final string.\n",
    "    return full_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "470494cc-026c-431a-a9c6-5ff75e8689cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "label_room\n",
    "\n",
    "Look at an image, and determine what type of room it is.\n",
    "\n",
    "    Args:\n",
    "      full_description: The Azure AI generated description returned in `unpack_azure_result`.\n",
    "      \n",
    "    Returns:\n",
    "      The predicted label, as well as the prompt and completion tokens generated by\n",
    "      the prompt (the latter two will be used to track the estimated cost of the image label).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def label_room(full_description):\n",
    "\n",
    "    # The options from which to choose between.\n",
    "    room_options = [\n",
    "        \"Bedroom\",\"Living Room\",\"Kitchen\",\"Bathroom\",\"Dining Room\",\"Home Office\",\"Studio\",\"Loft\",\n",
    "        \"Entertainment Room\",\"Laundry\",\"Hallway\", \"Yard\",\"Garden\",\"Patio\",\"Balcony\",\"Terrace\",\n",
    "        \"Pool\",\"Deck\",\"Location Exterior\",\"Garage\", \"Waterfront View\",\"City View\",\"Mountain View\",\n",
    "        \"Countryside View\",\"Forest\",\"Other\"\n",
    "    ]\n",
    "    \n",
    "    # What if you made the option list tighter?\n",
    "    room_options = [\n",
    "        \"Bedroom\",\"Living Room\",\"Kitchen\",\"Bathroom\",\"Location Exterior\",\"Other\"\n",
    "    ]\n",
    "\n",
    "     # Make 5 attempts to complete the call.\n",
    "    for attempt in range(1,6):\n",
    "        try:\n",
    "            # Determine which label best fits the image.            \n",
    "            completion = client.chat.completions.create(\n",
    "              model=\"gpt-4o\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Pick the option from this list that best describes the picture. You must pick an option from this list. Only pick “Location Exterior” if it looks like the home structure itself is included in the picture. If you are unsure which option to pick, return Other. List: {}\".format(room_options)},\n",
    "                {\"role\": \"user\", \"content\": \"Picture description: {}\".format(full_description)}\n",
    "              ]\n",
    "            )\n",
    "        \n",
    "            # save the label, and the tokens required to generate the response.\n",
    "            label = completion.choices[0].message.content\n",
    "            completion_tokens = int(completion.usage.completion_tokens)\n",
    "            prompt_tokens = int(completion.usage.prompt_tokens)\n",
    "                    \n",
    "            # return values.\n",
    "            return label, completion_tokens, prompt_tokens\n",
    "\n",
    "         # If exception, try again (up to 5 times).\n",
    "        except Exception as e:\n",
    "            print('label room error')\n",
    "            print(e)\n",
    "            pass\n",
    "    \n",
    "    # If 5 attempts fail, return Nones.\n",
    "    return None, 0, 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b726c17",
   "metadata": {},
   "source": [
    "_This might be deemed redundant now that Microsoft Azure Vision AI is integrated into the solution._\n",
    "\n",
    "The next function gets a little more creative with its prompt engineering. We have a dictionary of prompts for each room type, with slightly different instructions for each one tailored to the room in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ffdfd1-aeda-4863-b705-625753436732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "describe_room\n",
    "\n",
    "Look at an image, and describe its quality.\n",
    "\n",
    "    Args:\n",
    "      url: The url to the Zillow listing photo being assessed.\n",
    "      label: The room type gpt labeled the photo as.\n",
    "      \n",
    "    Returns:\n",
    "      A 60~ word quality description, as well as the prompt and completion tokens generated by\n",
    "      the prompt (the latter two will be used to track the estimated cost of the image label).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def gpt_describe_room(url, label):\n",
    "\n",
    "    # Don't waste time if you sense a failure upstream.\n",
    "    if label == None:\n",
    "        return None, 0, 0\n",
    "    \n",
    "    # Dictionary of prompts for each label type.\n",
    "    quality_prompts = {\n",
    "        'Bedroom': \"In 60 words or less, describe how the quality of the bedroom of a home on Zillow in the provided picture looks to you. Consider factors such as lighting, space, age of the room, size of room, and overall comfort. Your description should make it clear if this image portrays a low, low-medium, medium, medium-high, high, or very high value home.\",\n",
    "        'Living Room': \"In 60 words or less, describe how the quality of the living room of a home on Zillow in the provided picture looks to you. Take into account elements like furniture, lighting, age of the room, size of room, and overall ambiance. Your description should make it clear if this image portrays a low, low-medium, medium, medium-high, high, or very high value home.\",\n",
    "        'Kitchen': \"In 60 words or less, describe how the quality of the kitchen of a home on Zillow in the provided picture looks to you. Consider factors such as appliances, storage, age of the room, size of the room, and finishes. Your description should make it clear if this image portrays a low, low-medium, medium, medium-high, high, or very high value home.\",\n",
    "        'Bathroom': \"In 60 words or less, describe how the quality of the bathroom of a home on Zillow in the provided picture looks to you. Take into consideration cleanliness, fixtures, age of the room, size of the room, finishes,  and overall design. Your description should make it clear if this image portrays a low, low-medium, medium, medium-high, high, or very high value home.\",\n",
    "        'Location Exterior': \"In 60 words or less, describe how the quality of the exterior location of a home on Zillow in the provided picture looks to you.  Consider factors such as curb appeal, landscaping, size, how well maintained the building appears, and overall exterior aesthetics. Your description should make it clear if this image portrays a low, low-medium, medium, medium-high, high, or very high value home.\"\n",
    "    }    \n",
    "    \n",
    "    # If label is other, don't attempt to describe the quality! Saves a lot of utilization.\n",
    "    if label == 'Other':\n",
    "        return None, 0, 0\n",
    "    \n",
    "    # Select the appropriate prompt.\n",
    "    selected_prompt = quality_prompts[label]\n",
    "\n",
    "    # Make 5 attempts to complete the call.\n",
    "    for attempt in range(1,6):\n",
    "        try:\n",
    "            # Describe the quality of the labeled image.\n",
    "            response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "              {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": selected_prompt},\n",
    "                    {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                        \"url\": url,\n",
    "                        \"detail\": \"high\"\n",
    "                      },\n",
    "                    },\n",
    "                  ],\n",
    "                }\n",
    "              ],\n",
    "              max_tokens=100,\n",
    "            )\n",
    "            # save the description, and the tokens required to generate the response.\n",
    "            description = response.choices[0].message.content\n",
    "            completion_tokens = int(response.usage.completion_tokens)\n",
    "            prompt_tokens = int(response.usage.prompt_tokens)\n",
    "            \n",
    "            #print(description)\n",
    "        \n",
    "            # return values\n",
    "            return description, completion_tokens, prompt_tokens\n",
    "\n",
    "        # If exception, try again (up to 5 times).\n",
    "        except Exception as e:\n",
    "            print('describe room error')\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "    # If 5 attempts fail, return Nones.\n",
    "    return None, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086378bc",
   "metadata": {},
   "source": [
    "The final one converts the quality description into  a 1-10 ranking. This one uses `gpt4` since we're dealing with a text to text artificial intelligence task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ca0890-c90b-419a-b287-05928c6d997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "rank_room\n",
    "\n",
    "Look at an image, and describe its quality.\n",
    "\n",
    "    Args:\n",
    "      full_description: The gpt generated description of the room.\n",
    "      label: The room type gpt labeled the photo as.\n",
    "      \n",
    "    Returns:\n",
    "      A quality rating (1-10 scale), as well as the prompt and completion tokens generated by\n",
    "      the prompt (the latter two will be used to track the estimated cost of the image label).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def rank_room(full_description, label):\n",
    "\n",
    "    # Don't waste time if you sense a failure upstream.\n",
    "    if full_description == None or label == None:\n",
    "        return None, 0, 0\n",
    "    \n",
    "    \n",
    "    # Dictionary of prompts for each label type.\n",
    "    rating_prompts = {\n",
    "    'Bedroom': \"On a scale of 1-10, rate the quality of this bedroom based on this text description. Consider factors such as lighting, space, age of the room, size of the room, and overall comfort and perceived value of the home. Only impressive rooms of high or very high value get an 8 or higher. You must return, and only return, an integer between 1 and 10.\",\n",
    "    'Living Room': \"On a scale of 1-10, rate the quality of this living room based on this text description. Take into account elements like furniture, lighting, age of the room, size of the room, and overall ambiance and perceived value of the home. Only impressive rooms of high or very high value get an 8 or higher. You must return, and only return, an integer between 1 and 10.\",\n",
    "    'Kitchen': \"On a scale of 1-10, rate the quality of this kitchen based on this text description. Consider factors such as appliances, storage, age of the room, size of the room, finishes, and perceived value of the home. Only impressive rooms of high or very high value get an 8 or higher. You must return, and only return, an integer between 1 and 10.\",\n",
    "    'Bathroom': \"On a scale of 1-10, rate the quality of this bathroom based on this text description. Take into consideration cleanliness, fixtures, age of the room, size of the room, finishes, and overall design and perceived value of the home. Only impressive rooms of high or very high value get an 8 or higher. You must return, and only return, an integer between 1 and 10.\",\n",
    "    'Location Exterior': \"On a scale of 1-10, rate the quality of this exterior location based on this text description. Consider factors such as curb appeal, landscaping, how well maintained the building appears, size, and overall exterior aesthetics and perceived value of the home. Only impressive exteriors of high or very high value get an 8 or higher. You must return, and only return, an integer between 1 and 10.\"\n",
    "    }\n",
    "    \n",
    "    if label == 'Other':\n",
    "        return None, 0, 0\n",
    "\n",
    "    # Selec the appropriate prompt.\n",
    "    selected_prompt = rating_prompts[label] + ' Description: ' + full_description\n",
    "    \n",
    "    # Make 5 attempts to complete the call.\n",
    "    for attempt in range(1,6):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "              model=\"gpt-4o\",\n",
    "              messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You determine quality of different photos of a property based on a text description generated by Microsoft Azure Vision. You are biased toward giving lower scores -- don't give away high scores for just anything! If a home seems shabby, dated, or quaint - this is a lower score. If it seems luxurious, high end -- this is a higher score. Just return an integer between 1 and 10 based on the text description provided - nothing else. You must complete the task.\"},\n",
    "                {\"role\": \"user\", \"content\": selected_prompt}\n",
    "              ]\n",
    "            )\n",
    "\n",
    "            # save the rating, and the tokens required to generate the response.\n",
    "            rating = int(completion.choices[0].message.content)\n",
    "            completion_tokens = int(completion.usage.completion_tokens)\n",
    "            prompt_tokens = int(completion.usage.prompt_tokens)\n",
    "        \n",
    "            # return values\n",
    "            return rating, completion_tokens, prompt_tokens\n",
    "            \n",
    "        # If exception, try again (up to 5 times).\n",
    "        except Exception as e:\n",
    "            print('rank room error')\n",
    "            print(e)\n",
    "            time.sleep(60)\n",
    "            pass\n",
    "\n",
    "    # If 5 attempts fail, return Nones.\n",
    "    return None, 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd51920-8e4b-4ccb-8161-757ccbce81ab",
   "metadata": {},
   "source": [
    "### Step 2.2: Track spend\n",
    "\n",
    "OpenAI provides tools on their website to track spend -- but I think it is also helpful to keep track of it here. Using the price data available, this function takes in the prompt and completion tokens (or, said another way, the input and output tokens) and determines the cost of the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da4bbc1-dfa7-47eb-961d-d685ce584034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "calculate_spend\n",
    "\n",
    "Determine how much money was spent on an API call.\n",
    "\n",
    "    Args:\n",
    "      completion: the number of tokens outputted by ChatGPT to complete the task.\n",
    "      prompt: th enumber of tokens inputted to instruct ChatGPT on the task.\n",
    "      \n",
    "    Returns:\n",
    "      The cost in USD of the completion and prompt tokens given to the prompt.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def calculate_spend(completion, prompt):\n",
    "    \n",
    "    # Renaming the variables to the naming used in a billing context.\n",
    "    input_ = prompt\n",
    "    output = completion\n",
    "\n",
    "    # Input (prompt) cost: $2.50 / 1000000 tokens.\n",
    "    # Output (prompt) cost: $10.00 / 1000000 tokens.\n",
    "    cost = (input_ / 1000000 * 2.50) + (output / 1000000 * 10.00)\n",
    "\n",
    "    # Return cost.\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237391b9-8d72-41d7-9f0b-96967689ac46",
   "metadata": {},
   "source": [
    "### Step 2.3: Standardize, transform, and save image.\n",
    "\n",
    "Looking ahead to the Convolutional Neural Network(s) that I will train, I would like to have these images available locally to reference as I build the model. I was also take the time transform the images to make uniformly sized. We'll use shrinking and padding to accomplish this.\n",
    "\n",
    "First, we'll transform the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30117033-f833-48cb-beb7-ef846e7ef7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "transform_image\n",
    "\n",
    "Take a Zillow listing image and convert it into a uniformly sized image.\n",
    "\n",
    "    Args:\n",
    "      url: The url where a photo resides.\n",
    "      target_size: the pixel length x width size of the image after it is transformed.\n",
    "      \n",
    "    Returns:\n",
    "      A resized, padded, and cropped photo that matches the target_size.\n",
    "\"\"\"\n",
    "\n",
    "def transform_image(url, target_size=150):\n",
    "    try:\n",
    "        # Download the image from the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for unsuccessful responses\n",
    "        image_data = BytesIO(response.content)\n",
    "\n",
    "        # Open the image using Pillow\n",
    "        image = Image.open(image_data)\n",
    "\n",
    "        # Calculate the target size and the ratio\n",
    "        target_width, target_height = (target_size, target_size)\n",
    "        width_ratio = target_width / image.width\n",
    "        height_ratio = target_height / image.height\n",
    "\n",
    "        # Determine the new dimensions after resizing and cropping\n",
    "        if width_ratio < height_ratio:\n",
    "            new_width = target_width\n",
    "            new_height = int(image.height * width_ratio)\n",
    "        else:\n",
    "            new_width = int(image.width * height_ratio)\n",
    "            new_height = target_height\n",
    "\n",
    "        # Resize the image to fit within the new dimensions while maintaining the aspect ratio\n",
    "        resized_image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Randomly crop the resized image to the target size\n",
    "        left = random.randint(0, max(0, new_width - target_width))\n",
    "        top = random.randint(0, max(0, new_height - target_height))\n",
    "        right = left + target_width\n",
    "        bottom = top + target_height\n",
    "\n",
    "        cropped_image = resized_image.crop((left, top, right, bottom))\n",
    "\n",
    "        return cropped_image\n",
    "\n",
    "    except Exception as e:\n",
    "        print('transform_image',e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4b105",
   "metadata": {},
   "source": [
    "Then, we'll create a function for downloading that image for reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bccab428-fa66-4617-b751-00feea8b1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "download_image\n",
    "\n",
    "Save the image locally, and return its path.\n",
    "\n",
    "    Args:\n",
    "      image: the transformed image.\n",
    "      \n",
    "    Returns:\n",
    "      image_path: the path where the image was saved locally.\n",
    "\"\"\"\n",
    "\n",
    "def download_image(image):\n",
    "    try:\n",
    "        # generate image name.\n",
    "        characters = string.ascii_letters + string.digits\n",
    "        image_name = ''.join(random.choice(characters) for _ in range(32))\n",
    "\n",
    "        # Save the transformed image in the \"images\" folder\n",
    "        image_path = os.path.join(\"data/labeled_images/2 - images\", f\"\"\"{image_name}.png\"\"\")\n",
    "        image.save(image_path)\n",
    "\n",
    "        return image_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print('download_image',e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a43e3-c5fa-4b75-bac1-bcf726447eda",
   "metadata": {},
   "source": [
    "### Step 3: Build GPT Classifier for image labeling\n",
    "\n",
    "Let's now start to string together all of the pieces into one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd6f33a-2d81-48d3-8437-4b634e4d2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "gpt_image_classifier\n",
    "\n",
    "Walks the full path from a Zillow listing photo to its predicted classification.\n",
    "\n",
    "    Args:\n",
    "      row: A pandas Series generated from a DataFrame.iterrows() loop.\n",
    "      \n",
    "    Returns:\n",
    "      A pandas DataFrame with the Zillow listing photo, its prediction, the cost to \n",
    "      label it, and a location to the transformed image.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def gpt_image_classifier(row):\n",
    "\n",
    "    # Pull values out of row.\n",
    "    url = row['photo']\n",
    "    zillowId = row['zillowId']\n",
    "    \n",
    "    #print(zillowId, url)\n",
    "\n",
    "    # Track spend.\n",
    "    completion_tokens = 0\n",
    "    prompt_tokens = 0\n",
    "    \n",
    "    # Generate Azure description.\n",
    "    azure_result = azure_describe_room(url=url)\n",
    "    azure_description = unpack_azure_results(result=azure_result)\n",
    "    \n",
    "    # Generate label.\n",
    "    label, completion, prompt = label_room(full_description=azure_description)\n",
    "    completion_tokens += completion\n",
    "    prompt_tokens += prompt\n",
    "    \n",
    "    # Generate description.\n",
    "    gpt_description, completion, prompt = gpt_describe_room(url=url, label=label)\n",
    "    completion_tokens += completion\n",
    "    prompt_tokens += prompt\n",
    "    \n",
    "    # Generate ranking, 1-10, using gpt_description.\n",
    "    rank, completion, prompt = rank_room(full_description=gpt_description, label=label)\n",
    "    completion_tokens += completion\n",
    "    prompt_tokens += prompt\n",
    "    \n",
    "    # Calculate cost of classification.\n",
    "    cost = calculate_spend(completion=completion_tokens, prompt=prompt_tokens)\n",
    "\n",
    "    # Save image for reference.\n",
    "    image = transform_image(url=url)\n",
    "    image_path = download_image(image=image)\n",
    "    \n",
    "    # Put into pandas dataframe.\n",
    "    df = pd.DataFrame(data={\n",
    "        'zillowId':[zillowId],\n",
    "        'url':[url],\n",
    "        'image_path':[image_path],\n",
    "        'label':[label],\n",
    "        'rank':[rank],\n",
    "        'gpt_description':[gpt_description],\n",
    "        'azure_description': [azure_result],\n",
    "        'cost':[cost],\n",
    "        'completion_tokens':[completion_tokens],\n",
    "        'prompt_tokens':[prompt_tokens]\n",
    "    },index=[0])\n",
    "\n",
    "    # Return results\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5a421-0781-4774-8960-0afb1d0246fa",
   "metadata": {},
   "source": [
    "With the ability to now process one photo end to end, let's set it up so we can do that at scale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950b6f34-9d2a-4fe7-b04f-2c221dfd34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_data_to_csv\n",
    "\n",
    "A helper function to save data as it is processed.\n",
    "\n",
    "    Args:\n",
    "      data: The labeled image data.\n",
    "      existing_data: Whether or not there is existing data to append this to.\n",
    "      filename: The name of the file that should be created to store the data.\n",
    "      \n",
    "    Returns:\n",
    "      The DataFrame in its current state with the images processed.\n",
    "\"\"\"\n",
    "\n",
    "def save_data_to_csv(data, existing_data=None, filename='gpt_image_labels.csv'):\n",
    "\n",
    "    # If a pandas DataFrame is not passed through for existing_data, just save the data.\n",
    "    if isinstance(existing_data, pd.DataFrame) == False:\n",
    "        data.to_csv(filename, index=False)\n",
    "\n",
    "    # If there is existing data, append `data` to the existing data, and save it.\n",
    "    elif isinstance(existing_data, pd.DataFrame) == True:\n",
    "        data = pd.concat([existing_data, data])\n",
    "        data.to_csv(filename, index=False)\n",
    "    \n",
    "    # Either way -- whatever is in this csv is now the source of truth. Read it in as\n",
    "    # `existing_data, to which future loops will append.\n",
    "    existing_data = pd.read_csv(filename, index_col=False)\n",
    "\n",
    "    return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b28ab1b9-cbbf-4956-97da-66d282b3eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classify_images\n",
    "\n",
    "The function that will loop through a set number of photos, saving results intermittently.\n",
    "\n",
    "    Args:\n",
    "      numberPictures: the number of pictures we want to pay ChatGPT to label.\n",
    "      chunkSize: how many photos at a time should we process before saving results.\n",
    "      df: the DataFrame that we will fetch our images from (this will just be `photos_df`).\n",
    "      existing_data: Is there an existing DataFrame to work from? (Helpful if the function is interrupted).\n",
    "      \n",
    "    Returns:\n",
    "      The DataFrame of results.\n",
    "\"\"\"\n",
    "\n",
    "def classify_images(numberPictures, chunkSize, df=photos_df, existing_data=None):\n",
    "\n",
    "    # For capturing the values.\n",
    "    imageDataList = []\n",
    "\n",
    "    # For each row in the provided DataFrame, filtered for the number of pictures we wish to tag.\n",
    "    for i, row in tqdm(df.sample(frac=1)[:numberPictures].iterrows(), total=numberPictures, desc='Generating image labels'):\n",
    "        try:\n",
    "            # Classify image\n",
    "            one_image = gpt_image_classifier(row)\n",
    "            \n",
    "            #print('one_image')\n",
    "            #print(one_image)\n",
    "\n",
    "            # Save transformed image.\n",
    "            imageDataList.append(one_image)\n",
    "        \n",
    "        # Sometimes one of the 3 GPT prompts might fail. That's okay.\n",
    "        # We'll just move on and try the next image.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        # Only call this block in intervals that matches every chunk size.\n",
    "        if i % chunkSize == 0:\n",
    "            \n",
    "            # Concatenate all the data so far, and save it. If there is existing data, it will concatenate with that.\n",
    "            if len(imageDataList) > 0:\n",
    "                data = pd.concat(imageDataList)\n",
    "                existing_data = save_data_to_csv(data=data, existing_data = existing_data)\n",
    "\n",
    "            # Clear out the list since we've already written the data from this chunk.\n",
    "            imageDataList.clear()\n",
    "\n",
    "    # Finish up writing data for any values that weren't included in the final chunk (ex: if there are\n",
    "    # 120 pictures with a chunkSize of 50, this would capture the last 20.\n",
    "    if len(imageDataList) > 0:\n",
    "        data = pd.concat(imageDataList)\n",
    "        existing_data = save_data_to_csv(data=data, existing_data = existing_data)\n",
    "\n",
    "    return existing_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f177abc",
   "metadata": {},
   "source": [
    "### Step 4: Run it!\n",
    "\n",
    "Based on some testing done, I expect 10,000 images will cost about $100 (+/- $10 USD). That is about as much as I'd like to spend for a project like this, as I can always bolster the dataset with manual tagging. But given that I can tag about 500 photos in an hour, this represents 20~ hours of work I no longer have to do (So I am paying ChatGPT about $5 USD per hour of my life back). I can live with that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea33be8-3ac0-4d2f-9147-e868c08aea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_image_labels = classify_images(numberPictures=60000, chunkSize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977e652d-a249-4ca6-ad66-8298dc081ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_image_labels.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
