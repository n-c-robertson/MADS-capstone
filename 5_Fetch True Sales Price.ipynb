{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51307f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scraping and wrangling data.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "#from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "#from itertools import chain\n",
    "#import re\n",
    "\n",
    "# For building human like randomness into the webscraper.\n",
    "import time\n",
    "import random\n",
    "\n",
    "# For tracking progress.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41aaf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the listings file of webscraped listing we want to reference.\n",
    "listings = pd.read_csv('data/backup/BACKUP cleaned_zillow_data.csv').drop(columns=['Unnamed: 0']).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e628b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of listings: 128960\n"
     ]
    }
   ],
   "source": [
    "print(f'''# of listings: {len(listings)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df6fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_data = pd.read_csv('sales_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ff5ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = listings[~listings['zillowId'].isin([x for x in existing_data['zillowId']])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec4d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of listings (filtered): 127494\n"
     ]
    }
   ],
   "source": [
    "print(f'''# of listings (filtered): {len(listings)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1446cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for a more human looking request when web scraping.\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cookie': 'OptanonAlertBoxClosed=NR; OptanonAlertBoxClosed=NR; bev=1671467614_NDg3YWI1OWViODE0; country=UK; everest_cookie=1671467934.o8UVAqq76D3QPRXxo5SV.2TXJ26qIr6wRp2aGPSqpNu0hmcaL7JhFzmDBWbnzmDw; _csrf_token=V4%24.airbnb.com%24Dwa4m1DnRds%24l_ThNsfxVtYzJnV3gf7vehcjPJusOG_QBcYxaP2lZOM%3D; flags=0; _gcl_au=1.1.1979572412.1671467936; _ga=GA1.1.923589191.1671467936; tzo=-420; frmfctr=wide; ak_bmsc=2BF62A04339B5A8040F6BE42D7397A3E~000000000000000000000000000000~YAAQjuXEF4UQhgeFAQAA27JjLRJCqxbQs87wSwEdHRYX0G6/TDKZTovu4o6yCZ81IqZ6MhWckkFsgOG1eRvCyGcwYcGwZT6Hj0lKPA7+LECJW/p7uLVBPkdj9vDMc+M3APPxtp3dA4N7Xiz9k2CsK1cScWB15o99WkNT8TblkPRJMP+//tibMWG2xoEjeEV14znNbpxu/a19eOVXAFS6pvZFEKc1PvKaN6qVTNMeaHczy8vtIlUIhOHm8n0/neIB5WjhQewZ2bsI4Jk5FAyuugtHiXFiRJU7poWmTC1z3vQV5BNZly4umCoBfxL+4/1Gwtvg+5jzWIgEsQIOlNvhj6IhTWNdgIiUubyhm6SwXUHdD2r80FCzSqqkgs/F95M/C2eTuiMmpV414Qo=; jitney_client_session_id=d94ad55f-160c-4b75-aa19-6b94073c92fc; jitney_client_session_created_at=1671505980; _user_attributes=%7B%22curr%22%3A%22USD%22%2C%22guest_exchange%22%3A1.0%2C%22device_profiling_session_id%22%3A%221671467934--953d4ec0c3e885ffcfcecdcb%22%2C%22giftcard_profiling_session_id%22%3A%221671503786--bedf36f5fd7bc1727463feba%22%2C%22reservation_profiling_session_id%22%3A%221671503786--104feb525ef2701da84b3359%22%7D; jitney_client_session_updated_at=1671505982; _ga_2P6Q8PGG16=GS1.1.1671505983.4.1.1671505983.0.0.0; _uetsid=a14c3a907fbb11ed80888db3ffb5b814; _uetvid=a14c54c07fbb11edbfa35d16156e96a6; previousTab=%7B%22id%22%3A%22c5e39694-2720-411d-bbb4-5880ad1455b6%22%2C%22url%22%3A%22https%3A%2F%2Fwww.airbnb.com%2Fs%2FUnited-States%2Fhomes%22%7D; bm_sv=67E8D97D583DC2BA73262B2A33B8BD6A~YAAQjuXEF3gehweFAQAAzlqFLRKkxd6iaP1xq0hHbfO3CsyIIQifNN1kg5O8tYdobGiFUFkAU0ek5Tg67B28nu2btfk+1yHhT6TE1jibTRrmUjxmCnS6gZ9Sclthu91yIEEePEd32A66YE2G6ofZjSQCTS8TsZU9O8pKFQ7Nid5vVQ6DlyEPLX5yEYMLK94A1kBNulzVg+KjaWf2UN8pSkr66hIUxNn5Qq3fQtl8kCxJeSb6MGW977OC++DGDRXKqg==~1; cfrmfctr=MOBILE; cbkp=2',\n",
    "    'device-memory': '8',\n",
    "    'dpr': '2',\n",
    "    'ect': '4g',\n",
    "    'sec-ch-ua': '\"Not?A_Brand\";v=\"8\", \"Chromium\";v=\"108\", \"Google Chrome\";v=\"108\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'no-cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "    'viewport-width': '895'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae40a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "get_soup\n",
    "\n",
    "Our basic building blod of requesting the HTML of a page living at a given URL,\n",
    "and then turning that into a BeautifulSoup object.\n",
    "\n",
    "    Args:\n",
    "      url: the url of the page to scrape.\n",
    "      headers: the headers for the request (helps the request look less suspicious).\n",
    "    \n",
    "    Returns:\n",
    "      bs4 BeautifulSoup text representation of the page (or, None of fails 5 times in a row).\n",
    "\"\"\"\n",
    "\n",
    "def get_soup(url, headers=headers):\n",
    "\n",
    "    # Make 5 attempts to get the page.\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(headers)\n",
    "            response = session.get(url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup\n",
    "        except Exception as e:\n",
    "            print('get_soup:', e)\n",
    "            time.sleep(random.randint(5,11)) # Wait a few seconds before tryin again.\n",
    "            pass\n",
    "            \n",
    "    # If nothing returned, let it pass.\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881e8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fetch_script_data\n",
    "\n",
    "Helps us quickly isolate the section of the soup object where we \n",
    "will want to scrape data.\n",
    "\n",
    "    Args:\n",
    "      A bs4 BeautifulSoup text represenation of a listing page.\n",
    "    \n",
    "    Returns:\n",
    "      A specific subsection of the webpage where a lot of valuable metadata\n",
    "      lives. This is a nifty helper function for `scrape_listing_page`.\n",
    "\"\"\"\n",
    "\n",
    "def fetch_script_data(soup):\n",
    "    # JSON data hiding in script format in HTML.\n",
    "    script_id= \"__NEXT_DATA__\"\n",
    "    dictionary = json.loads(json.loads(soup.find('script', {'id':script_id}).string)['props']['pageProps']['componentProps']['gdpClientCache'])\n",
    "    \n",
    "    # The values we want to access are in the key at the 0th index -- but the key name changes based on the\n",
    "    # listing. As opposed to trying to recreate the key, we're just going to tell the script to \"get the values\n",
    "    # at the 0th index key.\n",
    "    _, dictionary_values = next(iter(dictionary.items()))\n",
    "    \n",
    "    # Fetch the property data.\n",
    "    script_data = dictionary_values['property']\n",
    "\n",
    "    # Return it as the variable script_data.\n",
    "    return script_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afb899f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "extract_sold_data\n",
    "\n",
    "If the listing has been sold, determine when it was sold, what the sell price was, and what the new \n",
    "zestimate is.\n",
    "\n",
    "    Args:\n",
    "        a JSON data struture `script_data`, output of the function `fetch_script_data`.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame with the zpid, price, zestimate, and date sold. If the listing is not yet\n",
    "        sold, the dataframe will be empty.\n",
    "\"\"\"\n",
    "\n",
    "def extract_sales_data(script_data):\n",
    "    \n",
    "    # Empty dictionary for collecting data.\n",
    "    scrape = {}\n",
    "    \n",
    "    # If the listing is Sold, attempt to scrape the data.\n",
    "    if script_data['homeStatus'] in ['SOLD','RECENTLY_SOLD']:\n",
    "        \n",
    "        try:\n",
    "            scrape['zillowId'] = script_data['zpid']\n",
    "        except:\n",
    "            scrape['zillowId'] = [None]\n",
    "        \n",
    "        try:\n",
    "            scrape['homeStatus'] = script_data['homeStatus']\n",
    "        except:\n",
    "            scrape['homeStatus'] = [None]\n",
    "        \n",
    "        try:\n",
    "            scrape['price'] = script_data['price']\n",
    "        except:\n",
    "            scrape['price'] = [None]\n",
    "            \n",
    "        try:\n",
    "            scrape['zestimate'] = script_data['zestimate']\n",
    "        except:\n",
    "            scrape['zestimate'] = [None]\n",
    "            \n",
    "        try:\n",
    "            scrape['dateSoldString'] = script_data['dateSoldString']\n",
    "        except:\n",
    "            scrape['dateSoldString'] = [None]\n",
    "        \n",
    "        # Save it as a DataFrame.\n",
    "        df = pd.DataFrame(scrape, index=[0])\n",
    "        \n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return an empty DataFrame.\n",
    "        return pd.DataFrame(data={\n",
    "            'zillowId': script_data['zpid'],\n",
    "            'homeStatus': script_data['homeStatus'],\n",
    "            'price': [None],\n",
    "            'zestimate': [None],\n",
    "            'dateSoldString': [None]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed74f734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "save_data_to_csv\n",
    "\n",
    "A helper function to save data as it is processed.\n",
    "\n",
    "    Args:\n",
    "      data: The sales data.\n",
    "      existing_data: Whether or not there is existing data to append this to.\n",
    "      filename: The name of the file that should be created to store the data.\n",
    "      \n",
    "    Returns:\n",
    "      The DataFrame in its current state with the images processed.\n",
    "\"\"\"\n",
    "\n",
    "def save_data_to_csv(data, existing_data=None, filename='sales_data.csv'):\n",
    "\n",
    "    # If a pandas DataFrame is not passed through for existing_data, just save the data.\n",
    "    if isinstance(existing_data, pd.DataFrame) == False:\n",
    "        data.to_csv(filename, index=False)\n",
    "\n",
    "    # If there is existing data, append `data` to the existing data, and save it.\n",
    "    elif isinstance(existing_data, pd.DataFrame) == True:\n",
    "        data = pd.concat([existing_data, data])\n",
    "        data.to_csv(filename, index=False)\n",
    "    \n",
    "    # Either way -- whatever is in this csv is now the source of truth. Read it in as\n",
    "    # `existing_data, to which future loops will append.\n",
    "    existing_data = pd.read_csv(filename, index_col=False)\n",
    "\n",
    "    return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c8ef68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "build_url\n",
    "\n",
    "The cleaned data no longer has the url. We'll quickly rebuild that from the zpid to scrape the relevant data.\n",
    "\n",
    "    Args:\n",
    "        zpid: the Zillow ID of the listing.\n",
    "        \n",
    "    Returns:\n",
    "        A url of the listing's page that we can use to scrape the data.\n",
    "\"\"\"\n",
    "\n",
    "def build_url(zpid):\n",
    "    return f'''https://www.zillow.com/homedetails/{zpid}_zpid/'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4778291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scrape_listing_sales_data\n",
    "\n",
    "A function to check all listings in the dataset to see if they have sold, and scrape the appropriate data.\n",
    "\n",
    "    Args:\n",
    "        chunksize: how many records to process through before saving a file.\n",
    "        df: the listings dataframe to reference.\n",
    "        existing_data: Whether or not we need to point at an existing data set (useful\n",
    "                       if the job is interrupted).\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame of the sales data, which is automatically saved in the local directory.\n",
    "\"\"\"\n",
    "\n",
    "def scrape_listing_sales_data(chunkSize, df=listings, existing_data=None):        \n",
    "    \n",
    "    sales_data_list = []\n",
    "    \n",
    "    # Iterate through each listing.\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc='Checking to see which listings have sold...'):\n",
    "        \n",
    "        # Attempt to scrape data for one listing.\n",
    "        try:\n",
    "            url = build_url(zpid=int(row['zillowId']))      # Construct webscraping URL.\n",
    "            soup = get_soup(url)                            # Get bs4 object.\n",
    "            script_data = fetch_script_data(soup)           # Extract structured listing data.\n",
    "            sales_data = extract_sales_data(script_data)    # Extract relevant sales data.\n",
    "            sales_data_list.append(sales_data)              # Append the results to the list.\n",
    "            time.sleep(random.randint(5,15))                # Slow down so we don't get blocked by Zillow.\n",
    "            \n",
    "        # If there is an exception, print it and move on.\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(url)\n",
    "            time.sleep(random.randint(10,30))\n",
    "            pass\n",
    "        \n",
    "        # Only call this block in intervals that matches every chunk size.\n",
    "        if i % chunkSize == 0:\n",
    "            \n",
    "            # Concatenate all the data so far, and save it. If there is existing data, it will concatenate with that.\n",
    "            if len(sales_data_list) > 0:\n",
    "                data = pd.concat(sales_data_list)\n",
    "                existing_data = save_data_to_csv(data=data, existing_data = existing_data)\n",
    "\n",
    "            # Clear out the list since we've already written the data from this chunk.\n",
    "            sales_data_list.clear()\n",
    "            \n",
    "    \n",
    "    # Finish up writing data for any values that weren't included in the final chunk (ex: if there are\n",
    "    # 120 listings with a chunkSize of 50, this would capture the last 20.\n",
    "    if len(sales_data_list) > 0:\n",
    "        data = pd.concat(sales_data_list)\n",
    "        existing_data = save_data_to_csv(data=data, existing_data = existing_data)\n",
    "\n",
    "    return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c12d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking to see which listings have sold...:   0%|         | 1/127494 [00:10<358:12:51, 10.11s/it]"
     ]
    }
   ],
   "source": [
    "# Execute the job!\n",
    "results = scrape_listing_sales_data(chunkSize=100, df=listings, existing_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea38e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28282e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
